# simpleneuralnetwork
A simple neural network implementation

The neural network supports both ReLU and sigmoid activation functions. For ReLU, He-initialization is used for generating the initial weights. The models can be saved and loaded. The backpropagation training works well. 

However, the code is just straight forward with no attempt to make it efficient what-so-ever. 

The code for the application is very messy.





