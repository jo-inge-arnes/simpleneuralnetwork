# simpleneuralnetwork
A simple neural network implementation

Note that the MNIST-files in this repository is not really MNIST but a much more difficult dataset. The currently checked in learning rate in the console application is also very low.

The neural network supports both ReLU and sigmoid activation functions. For ReLU, He-initialization is used for generating the initial weights. The models can be saved and loaded. The backpropagation training works well. 

However, the code is just straight forward with no attempt to make it efficient what-so-ever. 

The code for the application is very messy.





